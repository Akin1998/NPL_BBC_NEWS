{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BBC_news.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7edGkpcsich-",
        "outputId": "f5509ae7-fbad-4ef1-9017-af690c4c1a26"
      },
      "source": [
        "#importing all the necessary library functions\n",
        "import numpy as np\n",
        "import nltk\n",
        "import sklearn\n",
        "import operator\n",
        "import requests\n",
        "import re\n",
        "import unicodedata\n",
        "import pandas as pd\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.corpus import stopwords\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qJs8G05il3D",
        "outputId": "c9342705-3cb2-445b-df9d-90ad0315740d"
      },
      "source": [
        "#do not run this cell if you are not using the dataset from google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CITzsU2MixUZ"
      },
      "source": [
        "#reading the files names present in the folder\n",
        "#please give the correct path of each folder\n",
        "#geting all the filenames from the appropriate folder\n",
        "import glob\n",
        "import os\n",
        "os.chdir(r'drive/My Drive/AML/ex2/business')\n",
        "Business_file_names = glob.glob('*.txt')#getting the file names of all files present in the business folder\n",
        "#once we use the \"chdir\" the current directory is changed to it (business folder in our case as we used it first in the above cell)\n",
        "#so we use \"..\" to go back once "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sn4yE6UWi4_y"
      },
      "source": [
        "os.chdir(r'../entertainment')#getting the file names of all files present in the entertainment folder\n",
        "entertainment_file_names = glob.glob('*.txt')\n",
        "os.chdir(r'../politics')#getting the file names of all files present in the politics folder\n",
        "politics_file_names = glob.glob('*.txt')\n",
        "os.chdir(r'../sport')#getting the file names of all files present in the sport folder\n",
        "sports_file_names = glob.glob('*.txt')\n",
        "os.chdir(r'../tech')#getting the file names of all files present in the tech folder\n",
        "tech_file_names = glob.glob('*.txt')\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PiHC0NsajHGq"
      },
      "source": [
        "#creating variables to store the respective text data present in the folder\n",
        "#if you read the files from any other source then make sure the name_txt variables below contain all the content of all txt files in a folder\n",
        "#e.g: the variable business_txt should contain all the content present in all txt files from business folder\n",
        "business_txt=\"\"\n",
        "entertainment_txt=\"\"\n",
        "politics_txt=\"\"\n",
        "sport_txt=\"\"\n",
        "tech_txt=\"\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLq6_bMtjLT-",
        "outputId": "fefd5b1b-9691-4830-9eee-b00d0cf1f3e2"
      },
      "source": [
        "#reading text from all the files in business folder.\n",
        "for name in Business_file_names:\n",
        "  f=open(\"/content/drive/My Drive/AML/ex2/business/\"+name,\"r\")#give the path for business folder\n",
        "  business_txt=business_txt+\"\\n\"+f.read()\n",
        "  f.close()\n",
        "print(\"completed reading from business folder\")\n",
        "#reading text from all the files in entertainment folder.\n",
        "for name in entertainment_file_names:\n",
        "  f=open(\"/content/drive/My Drive/AML/ex2/entertainment/\"+name,\"r\")#give the path for entertainment folder\n",
        "  entertainment_txt=entertainment_txt+\"\\n\"+f.read()\n",
        "  f.close()\n",
        "print(\"completed reading from entertainment folder\")\n",
        "#reading text from all the files in politics folder.\n",
        "for name in politics_file_names:\n",
        "  f=open(\"/content/drive/My Drive/AML/ex2/politics/\"+name,\"r\")#give the path for politics folder\n",
        "  politics_txt=politics_txt+\"\\n\"+f.read()\n",
        "  f.close()\n",
        "print(\"completed reading from politics folder\")\n",
        "#reading text from all the files in sport folder.\n",
        "for name in sports_file_names:\n",
        "  f=open(\"/content/drive/My Drive/AML/ex2/sport/\"+name,\"r\")#give the path for politics folder\n",
        "  try: #getting an exception due \"Â£\" symbol so using try catch\n",
        "    sport_txt=sport_txt+\"\\n\"+f.read()\n",
        "  except UnicodeDecodeError:\n",
        "    pass\n",
        "  f.close()\n",
        "print(\"completed reading from sport folder\")\n",
        "#reading text from all the files in tech folder.\n",
        "for name in tech_file_names:\n",
        "  f=open(\"/content/drive/My Drive/AML/ex2/tech/\"+name,\"r\")#give the path for tech folder\n",
        "  tech_txt=tech_txt+\"\\n\"+f.read()\n",
        "  f.close()\n",
        "print(\"completed reading from tech folder\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "completed reading from business folder\n",
            "completed reading from entertainment folder\n",
            "completed reading from politics folder\n",
            "completed reading from sport folder\n",
            "completed reading from tech folder\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXN6BC-w8-wT"
      },
      "source": [
        "#stopwords are used to remove irrelevant words from the list\n",
        "# First, we get the stopwords list from nltk\n",
        "stopwords=set(nltk.corpus.stopwords.words('english'))\n",
        "# We can add more words to the stopword list, like punctuation marks\n",
        "stopwords.add(\".\")\n",
        "stopwords.add(\",\")\n",
        "stopwords.add(\"--\")\n",
        "stopwords.add(\"``\")\n",
        "stopwords.add(\" \")\n",
        "stopwords.add(\"'.'\")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHcQ52qgjbBt"
      },
      "source": [
        "#creating a function to tokenize the words\n",
        "#reference from \"https://towardsdatascience.com/from-dataframe-to-n-grams-e34e29df3460\"\n",
        "def tokenwords(text):\n",
        "  wnl = nltk.stem.WordNetLemmatizer()\n",
        "  words = re.sub(r'[^\\w\\s]', '', text).split()\n",
        "  return [wnl.lemmatize(word) for word in words if word not in stopwords]#we are removing all the stopwords before tokenizing\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rccxc0BFmG7j"
      },
      "source": [
        "words = tokenwords(business_txt)#tokenizing the words from business_txt\n",
        "business_words = pd.Series(nltk.ngrams(words, 1))# adding unigrams to business words\n",
        "business_words = business_words.append(pd.Series(nltk.ngrams(words, 2)))#adding bigrams to business words"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jpomf-em9UE"
      },
      "source": [
        "#tokenizing and adding unigrams and bigrams for entertainment_txt\n",
        "words = tokenwords(entertainment_txt)\n",
        "entertainment_words = pd.Series(nltk.ngrams(words, 1))\n",
        "entertainment_words = entertainment_words.append(pd.Series(nltk.ngrams(words, 2)))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lao-uXVgnuKz"
      },
      "source": [
        "#tokenizing and adding unigrams and bigrams for politics_txt\n",
        "words = tokenwords(politics_txt)\n",
        "politics_words = pd.Series(nltk.ngrams(words, 1))\n",
        "politics_words = politics_words.append(pd.Series(nltk.ngrams(words, 2)))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFCqt3gNn4pq"
      },
      "source": [
        "#tokenizing and adding unigrams and bigrams for sport_txt\n",
        "words = tokenwords(sport_txt)\n",
        "sport_words = pd.Series(nltk.ngrams(words, 1))\n",
        "sport_words = sport_words.append(pd.Series(nltk.ngrams(words, 2)))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "801E9yutoJp6"
      },
      "source": [
        "#tokenizing and adding unigrams and bigrams for tech_txt\n",
        "words = tokenwords(tech_txt)\n",
        "tech_words = pd.Series(nltk.ngrams(words, 1))\n",
        "tech_words = tech_words.append(pd.Series(nltk.ngrams(words, 2)))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFwE0MHD_h9Z"
      },
      "source": [
        "#creating a list that only holds nouns, pronouns, adverbs, preposition, adjective, conjuction, possive endings, interjection, foreign word(pos)\n",
        "business_words_pos=[]\n",
        "entertainment_words_pos=[]\n",
        "politics_words_pos=[]\n",
        "sport_words_pos=[]\n",
        "tech_words_pos=[]"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLU8LlEoBYUm"
      },
      "source": [
        "#reference from https://www.guru99.com/pos-tagging-chunking-nltk.html\n",
        "#{NN, NNS} = noun, {NNP,NNPS}= proper noun, {VB,VBG,VBD,VBN,VBP,VBZ} = verb, {RB,RBR,RBS}=adverb ,PRP = pronoun PRP$= Possive pronoun, POS = possive ending, IN=preposition\n",
        "#{JJ, JJR, JJS} = adjective, CC = conjunction, FW= foreign word, UH=interjection, \n",
        "for word in business_words:\n",
        "  if nltk.pos_tag(word)[0][1] == 'NN' or 'NNS' or 'VB' or 'VBG' or 'VBD' or 'VBN' or 'VBP' or 'VBZ' or 'RB' or 'RBR' or 'RBS' or 'PRP' or 'PRP$' or 'POS' or 'JJ' or 'JJR' or 'JJS' or 'CC' or 'FW' or 'UH':\n",
        "    business_words_pos.append(word)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "conIbuaRE1u-"
      },
      "source": [
        "for word in entertainment_words:\n",
        "  if nltk.pos_tag(word)[0][1] == 'NN' or 'NNS' or 'VB' or 'VBG' or 'VBD' or 'VBN' or 'VBP' or 'VBZ' or 'RB' or 'RBR' or 'RBS' or 'PRP' or 'PRP$' or 'POS' or 'JJ' or 'JJR' or 'JJS' or 'CC' or 'FW' or 'UH':\n",
        "    entertainment_words_pos.append(word)\n",
        "for word in politics_words:\n",
        "  if nltk.pos_tag(word)[0][1] == 'NN' or 'NNS' or 'VB' or 'VBG' or 'VBD' or 'VBN' or 'VBP' or 'VBZ' or 'RB' or 'RBR' or 'RBS' or 'PRP' or 'PRP$' or 'POS' or 'JJ' or 'JJR' or 'JJS' or 'CC' or 'FW' or 'UH':\n",
        "    politics_words_pos.append(word)\n",
        "for word in sport_words:\n",
        "  if nltk.pos_tag(word)[0][1] == 'NN' or 'NNS' or 'VB' or 'VBG' or 'VBD' or 'VBN' or 'VBP' or 'VBZ' or 'RB' or 'RBR' or 'RBS' or 'PRP' or 'PRP$' or 'POS' or 'JJ' or 'JJR' or 'JJS' or 'CC' or 'FW' or 'UH':\n",
        "    sport_words_pos.append(word)\n",
        "for word in tech_words:\n",
        "  if nltk.pos_tag(word)[0][1] == 'NN' or 'NNS' or 'VB' or 'VBG' or 'VBD' or 'VBN' or 'VBP' or 'VBZ' or 'RB' or 'RBR' or 'RBS' or 'PRP' or 'PRP$' or 'POS' or 'JJ' or 'JJR' or 'JJS' or 'CC' or 'FW' or 'UH':\n",
        "    tech_words_pos.append(word)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6gMQWbIGM5A"
      },
      "source": [
        "#cleaning the tokens\n",
        "#now our dataset contains values like this \"('word',)\" and we need to be like \"word\"\n",
        "temp=[]\n",
        "a=[]\n",
        "b=[]\n",
        "c=[]\n",
        "for word in business_words_pos:\n",
        "  temp.append(str(word).replace(\"', '\",\" \"))\n",
        "for word in temp:\n",
        "  a.append(str(word).replace(\"'\",\"\"))\n",
        "for word in a:\n",
        "  b.append(str(word).replace(\"(\",\"\"))\n",
        "for word in b:\n",
        "  c.append(str(word).replace(\")\",\"\"))\n",
        "business_words_pos=[]\n",
        "for word in c:\n",
        "  business_words_pos.append(str(word).replace(\",\",\"\"))\n",
        "#--------------------------------------------------------------------------------------------\n",
        "temp=[]\n",
        "a=[]\n",
        "b=[]\n",
        "c=[]\n",
        "for word in entertainment_words_pos:\n",
        "  temp.append(str(word).replace(\"', '\",\" \"))\n",
        "for word in temp:\n",
        "  a.append(str(word).replace(\"'\",\"\"))\n",
        "for word in a:\n",
        "  b.append(str(word).replace(\"(\",\"\"))\n",
        "for word in b:\n",
        "  c.append(str(word).replace(\")\",\"\"))\n",
        "entertainment_words_pos=[]\n",
        "for word in c:\n",
        "  entertainment_words_pos.append(str(word).replace(\",\",\"\"))\n",
        "#-----------------------------------------------------------------\n",
        "temp=[]\n",
        "a=[]\n",
        "b=[]\n",
        "c=[]\n",
        "for word in politics_words_pos:\n",
        "  temp.append(str(word).replace(\"', '\",\" \"))\n",
        "for word in temp:\n",
        "  a.append(str(word).replace(\"'\",\"\"))\n",
        "for word in a:\n",
        "  b.append(str(word).replace(\"(\",\"\"))\n",
        "for word in b:\n",
        "  c.append(str(word).replace(\")\",\"\"))\n",
        "politics_words_pos=[]\n",
        "for word in c:\n",
        "  politics_words_pos.append(str(word).replace(\",\",\"\"))\n",
        "#---------------------------------------------------------------\n",
        "temp=[]\n",
        "a=[]\n",
        "b=[]\n",
        "c=[]\n",
        "for word in sport_words_pos:\n",
        "  temp.append(str(word).replace(\"', '\",\" \"))\n",
        "for word in temp:\n",
        "  a.append(str(word).replace(\"'\",\"\"))\n",
        "for word in a:\n",
        "  b.append(str(word).replace(\"(\",\"\"))\n",
        "for word in b:\n",
        "  c.append(str(word).replace(\")\",\"\"))\n",
        "sport_words_pos=[]\n",
        "for word in c:\n",
        "  sport_words_pos.append(str(word).replace(\",\",\"\"))\n",
        "#----------------------------------------------------------------\n",
        "temp=[]\n",
        "a=[]\n",
        "b=[]\n",
        "c=[]\n",
        "for word in tech_words_pos:\n",
        "  temp.append(str(word).replace(\"', '\",\" \"))\n",
        "for word in temp:\n",
        "  a.append(str(word).replace(\"'\",\"\"))\n",
        "for word in a:\n",
        "  b.append(str(word).replace(\"(\",\"\"))\n",
        "for word in b:\n",
        "  c.append(str(word).replace(\")\",\"\"))\n",
        "tech_words_pos=[]\n",
        "for word in c:\n",
        "  tech_words_pos.append(str(word).replace(\",\",\"\"))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rmx7cCd311w1"
      },
      "source": [
        "# Now we create a frequency dictionary with all words in the dataset\n",
        "dict_word_frequency={}\n",
        "\n",
        "for word in business_words_pos:\n",
        "  #if word in stopwords: continue #since we already removed the stopwords we do not need this\n",
        "  if word not in dict_word_frequency: dict_word_frequency[word]=1\n",
        "  else: dict_word_frequency[word]+=1\n",
        "\n",
        "for word in entertainment_words_pos:\n",
        "  #if word in stopwords: continue\n",
        "  if word not in dict_word_frequency: dict_word_frequency[word]=1\n",
        "  else: dict_word_frequency[word]+=1\n",
        "\n",
        "for word in politics_words_pos:\n",
        "  #if word in stopwords: continue\n",
        "  if word not in dict_word_frequency: dict_word_frequency[word]=1\n",
        "  else: dict_word_frequency[word]+=1\n",
        "\n",
        "for word in sport_words_pos:\n",
        "  #if word in stopwords: continue\n",
        "  if word not in dict_word_frequency: dict_word_frequency[word]=1\n",
        "  else: dict_word_frequency[word]+=1\n",
        "\n",
        "for word in tech_words_pos:\n",
        "  #if word in stopwords: continue\n",
        "  if word not in dict_word_frequency: dict_word_frequency[word]=1\n",
        "  else: dict_word_frequency[word]+=1"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpIXtF_K3y_7",
        "outputId": "8bd44a7c-7d5f-4de3-fd8a-8dcecb1cf28f"
      },
      "source": [
        "# Now we create a sorted frequency list with the top 2000 words, using the function \"sorted\". Let's see the 15 most frequent words\n",
        "#only using 2000 words to make the code run faster. give more words if in a high spec pc\n",
        "sorted_list = sorted(dict_word_frequency.items(), key=operator.itemgetter(1), reverse=True)[:2000] #no.of words\n",
        "i=0\n",
        "for word,frequency in sorted_list[:15]:\n",
        "  i+=1\n",
        "  print (str(i)+\". \"+str(word)+\" - \"+str(frequency))\n",
        "print(len(sorted_list))\n",
        "# Finally, we create our vocabulary based on the sorted frequency list \n",
        "vocabulary=[]\n",
        "for word,frequency in sorted_list:\n",
        "  vocabulary.append(word)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1. The - 8012\n",
            "2. said - 7243\n",
            "3. I - 3251\n",
            "4. Mr - 3004\n",
            "5. year - 2763\n",
            "6. would - 2571\n",
            "7. also - 2102\n",
            "8. people - 1923\n",
            "9. But - 1787\n",
            "10. It - 1639\n",
            "11. US - 1568\n",
            "12. He - 1556\n",
            "13. one - 1535\n",
            "14. new - 1510\n",
            "15. could - 1504\n",
            "2000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3VWLSfjLAPc"
      },
      "source": [
        "#function to transform sentences to vectors\n",
        "def get_vector_text(list_vocab,string):\n",
        "  vector_text=np.zeros(len(list_vocab))\n",
        "  list_tokens_string=tokenwords(string)\n",
        "  for i, word in enumerate(list_vocab):\n",
        "    if word in list_tokens_string:\n",
        "      vector_text[i]=list_tokens_string.count(word)\n",
        "  return vector_text"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svs1deFXPXZ0"
      },
      "source": [
        "business_txt=business_txt.split(\"\\n\")\n",
        "entertainment_txt=entertainment_txt.split(\"\\n\")\n",
        "politics_txt=politics_txt.split(\"\\n\")\n",
        "sport_txt=sport_txt.split(\"\\n\")\n",
        "tech_txt=tech_txt.split(\"\\n\")"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AO3GayLALTIS"
      },
      "source": [
        "#creating x and y training sets, x contains the vectors and y contains the labels\n",
        "#the vocabulary is made using the features \"frequency\", \"n-grams(unigram and bigram)\" and Parts of speech(pos)\n",
        "X_train=[]\n",
        "Y_train=[]\n",
        "for business_words in business_txt:\n",
        "  business_review=get_vector_text(vocabulary,business_words)\n",
        "  X_train.append(business_review)\n",
        "  Y_train.append(0)\n",
        "\n",
        "for entertainment_words in entertainment_txt:\n",
        "  entertainment_review=get_vector_text(vocabulary,entertainment_words)\n",
        "  X_train.append(entertainment_review)\n",
        "  Y_train.append(1)\n",
        "\n",
        "for politics_words in politics_txt:\n",
        "  politics_review=get_vector_text(vocabulary,politics_words)\n",
        "  X_train.append(politics_review)\n",
        "  Y_train.append(2)\n",
        "\n",
        "for sport_words in sport_txt:\n",
        "  sport_review=get_vector_text(vocabulary,sport_words)\n",
        "  X_train.append(sport_review)\n",
        "  Y_train.append(3)\n",
        "\n",
        "for tech_words in tech_txt:\n",
        "  tech_review=get_vector_text(vocabulary,tech_words)\n",
        "  X_train.append(tech_review)\n",
        "  Y_train.append(4)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Ipq0x_rQ3NT"
      },
      "source": [
        "#importing chi2 and selecting 100 best features\n",
        "from sklearn.feature_selection import chi2\n",
        "from sklearn.feature_selection import SelectKBest"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCiXPjbVRDao"
      },
      "source": [
        "x_train_array = np.asarray(X_train)\n",
        "Y = np.asarray(Y_train)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGVALFu9RToZ"
      },
      "source": [
        "#only selecting 100 best features to make the code run faster, the result achieved might be lower than usual\n",
        "# to get a better result increase the value of k below\n",
        "analysis=SelectKBest(chi2, k=100).fit(x_train_array,Y)\n",
        "X = analysis.transform(x_train_array)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpXQC9K6R47l"
      },
      "source": [
        "#using test train split to split the data into 80% training set and 20%testing set\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_test,y_train,y_test = train_test_split(X,Y, train_size = 0.8, random_state = 1)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4n69nl_SSnwf"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCDGOKMQqVjI"
      },
      "source": [
        "#using svc classfire and crossvalidation\n",
        "#might take a long time depending on the pc specs\n",
        "#took about 3hrs 30 min in google colab for 2000 features\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn import svm\n",
        "clf = svm.SVC(kernel='linear', C=1, gamma='scale')\n",
        "clf.fit(X,Y)\n",
        "scores = cross_val_score(clf, X, Y, cv=5)\n"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fhxumb4WkSL7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adb4277d-3546-4f76-826a-41f53ed3a2e0"
      },
      "source": [
        "pre = clf.predict(x_test)\n",
        "print( confusion_matrix(y_test, pre))\n"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 385    7   29  662   18]\n",
            " [  21  286   12  511    4]\n",
            " [  30    7  385  631    8]\n",
            " [   5   10   10 1101   10]\n",
            " [  26   17   10  586  379]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-C5ILSnALH1k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2980db93-8adf-45dc-f288-774f972583a9"
      },
      "source": [
        "print(classification_report(y_test, pre))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.35      0.49      1101\n",
            "           1       0.87      0.34      0.49       834\n",
            "           2       0.86      0.36      0.51      1061\n",
            "           3       0.32      0.97      0.48      1136\n",
            "           4       0.90      0.37      0.53      1018\n",
            "\n",
            "    accuracy                           0.49      5150\n",
            "   macro avg       0.76      0.48      0.50      5150\n",
            "weighted avg       0.74      0.49      0.50      5150\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}